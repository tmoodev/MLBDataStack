{
  "metadata": {
    "name": "di_baseball_pitchdata",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\n#!/usr/bin/env python3\r\n\r\nimport pybaseball\r\nfrom pybaseball import statcast\r\nimport pandas as pd\r\nfrom sqlalchemy import create_engine, text\r\nfrom datetime import datetime, timedelta\r\n\r\n# Start the timer logcd\r\n\r\nstart_time \u003d datetime.now()\r\nprint(\u0027Pipeline Initiated\u0027)\r\n\r\ndflog \u003d pd.DataFrame(columns\u003d[\u0027pipeline\u0027, \u0027database\u0027, \u0027table\u0027, \u0027start_time\u0027, \u0027end_time\u0027, \u0027username\u0027])\r\n\r\nprint(\u0027Connecting to Database\u0027)\r\n\r\n# MySQL Database Connection Details\r\nserver_name \u003d \u002710.139.0.31\u0027\r\ndatabase_name \u003d \u0027baseball\u0027\r\nusername \u003d \u0027sqltravis\u0027\r\npassword \u003d \r\n\r\n# Create SQLAlchemy Engine for MySQL\r\nconnection_string \u003d f\u0027mysql+pymysql://{username}:{password}@{server_name}/{database_name}\u0027\r\nengine \u003d create_engine(connection_string)\r\n\r\nprint(\u0027Connection Successful\u0027)\r\n\r\n# Define table name\r\ntable_name \u003d \u0027PitchData\u0027\r\n\r\n# Determine the most recent loaded date in the database\r\nwith engine.connect() as conn:\r\n    result \u003d conn.execute(text(f\"SELECT MAX(game_date) FROM {table_name}\"))\r\n    last_loaded_date \u003d result.scalar()\r\n\r\n# Set default start date if no data is present\r\nif last_loaded_date is None:\r\n    start_date \u003d \u00272025-03-01\u0027  # Fallback start date\r\nelse:\r\n    start_date \u003d (last_loaded_date + timedelta(days\u003d1)).strftime(\u0027%Y-%m-%d\u0027)\r\n\r\n# Set end date as today\r\nend_date \u003d (datetime.today() - timedelta(days\u003d1)).strftime(\u0027%Y-%m-%d\u0027)  # Always yesterday\r\n\r\nprint(f\u0027Fetching data from {start_date} to {end_date}\u0027)\r\n\r\n# Get Data From StatCast\r\nbaseball_data \u003d statcast(start_dt\u003dstart_date, end_dt\u003dend_date)\r\n\r\n# If no new data, exit\r\nif baseball_data.empty:\r\n    print(\"No new data to load. Exiting.\")\r\nelse:\r\n    # Add insert_time column\r\n    baseball_data.insert(len(baseball_data.columns), \u0027insert_time\u0027, datetime.now())\r\n\r\n    # Append data to the existing table\r\n    baseball_data.to_sql(table_name, engine, if_exists\u003d\u0027append\u0027, index\u003dFalse)\r\n\r\n    # Capture end time\r\n    end_time \u003d datetime.now()\r\n\r\n    # Insert log data into DataFrame\r\n    new_row_data \u003d {\r\n        \u0027pipeline\u0027: \u0027savant-to-baseball\u0027,\r\n        \u0027database\u0027: database_name,\r\n        \u0027table\u0027: table_name,\r\n        \u0027start_time\u0027: start_time,\r\n        \u0027end_time\u0027: end_time,\r\n        \u0027username\u0027: username\r\n    }\r\n\r\n    # Append new row to the log DataFrame\r\n    dflog.loc[len(dflog)] \u003d new_row_data\r\n\r\n    # Define log table name\r\n    log_table_name \u003d \u0027PipelineLog\u0027\r\n\r\n    # Insert log data into MySQL table\r\n    dflog.to_sql(log_table_name, engine, if_exists\u003d\u0027append\u0027, index\u003dFalse)\r\n\r\n    print(f\u0027New data from {start_date} to {end_date} inserted into {table_name} successfully.\u0027)\r\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}