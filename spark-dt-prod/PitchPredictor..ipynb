{
  "metadata": {
    "name": "PitchPredictor",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pybaseball import statcast\nfrom pyspark.sql.functions import col, count, isnan, when, mean, stddev\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\n\n\n#Start Spark Session w. Memory Parameters\nspark \u003d SparkSession.builder \\\n    .appName(\"Baseball\") \\\n    .master(\"spark://10.139.0.30:7077\") \\\n    .getOrCreate()\n\n# Check if the SparkContext is alive\nprint(\"Spark UI:\", spark.sparkContext.uiWebUrl)\nprint(\"Master:\", spark.sparkContext.master)\nprint(\"Executors:\", spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Ingest raw parquet into Spark df\n\nraw_df \u003d spark.read.parquet(\"/home/travis/statcast_2015_2024.parquet\")\nraw_df.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Add an pk to the dataframe\ndf \u003d raw_df.withColumn(\"id\", monotonically_increasing_id())"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Remove deprecated columns\n\ncolumns_to_drop \u003d [\n    \"spin_rate_deprecated\", \"break_angle_deprecated\", \"break_length_deprecated\",\n    \"tfs_deprecated\", \"tfs_zulu_deprecated\"\n]\n\ncleaned_df \u003d df.drop(*columns_to_drop)\n\ncleaned_df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Identify numeric columns and count NaN values\n\n# Get all numeric columns \nnumeric_cols \u003d [f.name for f in cleaned_df.schema.fields if f.dataType.simpleString() in [\"int\", \"bigint\", \"double\", \"float\"]]\n\n# Count NaN values in all numeric columns\nnan_counts \u003d cleaned_df.select(\n    [count(when(isnan(col(c)), c)).alias(c) for c in numeric_cols]\n)\n\nnan_counts.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Count total rows in DataFrame\ntotal_rows \u003d cleaned_df.count()\n\n# Count NaN and NULL values for each numeric column\nnan_counts \u003d cleaned_df.select([\n    count(when(isnan(col(c)) | col(c).isNull(), c)).alias(c) for c in numeric_cols\n]).collect()[0].asDict()\n\n# Identify columns with more than 50% NaN values\nthreshold \u003d 0.5 * total_rows\ncolumns_to_drop \u003d [col_name for col_name, nan_count in nan_counts.items() if nan_count \u003e threshold]\n\n# Drop columns with excessive missing values\ncleaned_df \u003d cleaned_df.drop(*columns_to_drop)\n\n# Show the dropped columns\nprint(f\"Dropped columns: {columns_to_drop}\")\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT *\nFROM cleaned_df\nLIMIT(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n############################\n#Identify \u0026 Remove Outliers\n############################\n\n# Filter by domain knowledge for pitch speed:\n\ndf_no_outliers \u003d cleaned_df.filter(\n    (cleaned_df[\"pitch_speed\"] \u003e\u003d 40) \u0026 \n    (cleaned_df[\"pitch_speed\"] \u003c\u003d 105)\n)\n\n# Filter by standard deviation:\n\n stats \u003d df_no_outliers.select(\n     mean(col(\"pitch_speed\")).alias(\"mean_speed\"),\n     stddev(col(\"pitch_speed\")).alias(\"std_speed\")\n ).collect()[0]\n mean_speed \u003d stats[\"mean_speed\"]\n std_speed \u003d stats[\"std_speed\"]\n low_cutoff \u003d mean_speed - 3 * std_speed\n high_cutoff \u003d mean_speed + 3 * std_speed\n df_no_outliers \u003d df_no_outliers.filter(\n     (col(\"pitch_speed\") \u003e\u003d low_cutoff) \u0026 (col(\"pitch_speed\") \u003c\u003d high_cutoff)\n )\n\n# Reduce the number of columns to lighten df:\n\nfiltered_for_iqr \u003d df_no_outliers.select(\n    \"id\",\n    \"pitch_speed\",\n    \"release_spin_rate\",\n    \"plate_x\",\n    \"plate_z\",\n    \"release_extension\",\n    \"arm_angle\",\n    \"api_break_x_batter_in\",\n    \"api_break_x_arm\",\n    \"api_break_z_with_gravity\",\n    \"hyper_speed\",\n    \"delta_pitcher_run_exp\",\n    \"estimated_slg_using_speedangle\",\n    \"swing_length\",\n    \"bat_speed\",\n    \"spin_axis\",\n    \"pitch_number\",\n    \"launch_speed_angle\"\n    \"iso_value\",\n    \"babip_value\",\n    \"woba_denom\",\n    \"woba_value\",\n    \"estimated_woba_using_speedangle\",\n    \"estimated_ba_using_speedangle\",\n    \"release_pos_y\",\n    \"release_speed\",\n    \"release_pos_x\",\n    \"release_pos_z\",\n    \"spin_dir\",\n    \"zone\",\n    \"pfx_x\",\n    \"pfx_z\",\n    \"hc_x\",\n    \"hc_y\",\n    \"vx0\",\n    \"vy0\",\n    \"vz0\",\n    \"ax\",\n    \"ay\",\n    \"az\",\n    \"sz_top\",\n    \"sz_bot\",\n    \"release_spin_rate\",\n)\n\n# Compute IQR and filter outliers for each column\n\nfor c in numeric_cols:\n    quantiles \u003d df_filtered_for_iqr.approxQuantile(c, [0.25, 0.75], 0.0)\n    \n    # Ensure two quantile values exist before proceeding\n\n    if len(quantiles) \u003d\u003d 2:\n        Q1, Q3 \u003d quantiles\n        IQR \u003d Q3 - Q1\n        lower_bound \u003d Q1 - 1.5 * IQR\n        upper_bound \u003d Q3 + 1.5 * IQR\n        \n        # Apply filtering to remove outliers\n\n        outliers_removed \u003d df_filtered_for_iqr.filter((col(c) \u003e\u003d lower_bound) \u0026 (col(c) \u003c\u003d upper_bound))\n    else:\n        print(f\"Skipping column \u0027{c}\u0027 due to insufficient quantile data.\")\n\n# Show the cleaned DataFrame\n\noutliers_removed.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Rejoin dataset with outliers removed to attributes from df_cleaned\n#Create temp sql views\noutliers_removed.createOrReplaceTempView(\"removed_outliers\")\ncleaned_df.createOrReplaceTempView(\"cleaned\")\n\nready_for_index \u003d spark.sql(\"\"\"\nSELECT *\nFROM removed_outliers a\nLEFT JOIN cleaned b\n    ON a.id \u003d b.id\n\"\"\")\n\nready_for_index.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#Index the categorical pitch_type column\npitch_type_indexer \u003d StringIndexer(\n    inputCol\u003d\"pitch_type\", \n    outputCol\u003d\"pitch_type_index\"\n)\n\n# One-hot encode the indexed pitch_type\npitch_type_encoder \u003d OneHotEncoder(\n    inputCols\u003d[\"pitch_type_index\"], \n    outputCols\u003d[\"pitch_type_ohe\"]\n)\n\n# Fit the indexer on df_filtered and transform\nindexed_df \u003d pitch_type_indexer.fit(ready_for_index).transform(ready_for_index)\n\n# Fit the encoder on the indexed data and transform\nmodel_data \u003d pitch_type_encoder.fit(indexed_df).transform(indexed_df)\n\n# --\u003e encoded_df now contains a new vector column \u0027pitch_type_ohe\u0027\n#     that you can use downstream in modeling.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Save cleaned data\n\nindexed_df.write.mode(\"overwrite\").parquet(\"model_data.parquet\")\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}